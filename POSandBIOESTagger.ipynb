{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POSTagger.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "81R2Qp-Ix9G0",
        "Mz8HGMzeGuUl",
        "STpZk5qBG_Pv"
      ],
      "authorship_tag": "ABX9TyO3o23foQL8cnazm8dwnXyw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahinic/syntax-parsing-lstm/blob/master/POSandBIOESTagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFbv5_zVGCTs"
      },
      "source": [
        "# **POS Tagging with Penn Treebank dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDddHDpmm3wn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yoAdDT6xI01",
        "outputId": "2b86e992-960b-4b28-9eec-ae1385410665"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81R2Qp-Ix9G0"
      },
      "source": [
        "## 01. Penn Treebank dataset preparation\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWGu9temGM9b"
      },
      "source": [
        "# Penn Treebank English Dataset Preparation\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "class PennTreeBankDataset():\n",
        "\n",
        "    def load_file(self, filename:str): # given file names, returns each line\n",
        "        file = open(filename)\n",
        "        roh_daten = file.readlines()\n",
        "        return roh_daten\n",
        "\n",
        "    def preprocessing(self, file:str):\n",
        "        #step 1: fetch file contents\n",
        "        raw_dataset = self.load_file(file)\n",
        "\n",
        "        #step 2: form sentences and corresponding POS tags:\n",
        "        all_samples, all_labels = [], []\n",
        "        for sample in raw_dataset:\n",
        "\n",
        "            sample = sample.replace('\\n','')           \n",
        "            sentence_dirty = list(filter(None,sample.split(')'))) # spliiting on closing brackets and viewing a subset of the result \n",
        "            sentence_clean, tags_clean = [],[]\n",
        "            \n",
        "            for word in sentence_dirty:\n",
        "                word = word.replace(\"(\",\"\")\n",
        "                sentence_clean.append(word.split(' ')[-1])\n",
        "                tags_clean.append(word.split(' ')[-2]) if len(word) > 2 else ''\n",
        "            \n",
        "            if len(sentence_clean) != len(tags_clean):\n",
        "                print(\"Mismatch in no. of tokens in the line!\")\n",
        "                break\n",
        "            \n",
        "            # add these two to the big list\n",
        "            all_samples.append(sentence_clean) \n",
        "            all_labels.append(tags_clean)\n",
        "\n",
        "        if len(all_samples) != len(all_labels):\n",
        "            print(f\"Total no. of samples: {len(all_samples)} \\nTotal no. of POS tags: {len(all_labels)}\")\n",
        "            print(\"Mismatch in no. of lines\")\n",
        "            exit\n",
        "\n",
        "        return list(zip(all_samples,all_labels))\n",
        "\n",
        "    def export_files(self): # one time to create datasets\n",
        "\n",
        "        list_of_files = [\"PennTreeBankTrain.pklz\",\"PennTreeBankTest.pklz\",\"PennTreeBankValid.pklz\"]\n",
        "        list_of_files_tags = [\"PennTreeBankTrainPOS.pkl\",\"PennTreeBankTestPOS.pkl\",\"PennTreeBankValidPOS.pkl\"]\n",
        "        list_of_datasets = [\"02-21.10way.clean.txt\",\"23.auto.clean.txt\",\"22.auto.clean.txt\"]\n",
        "\n",
        "        for final,pos,raw in zip(list_of_files,list_of_files_tags,list_of_datasets):\n",
        "            # print(f\"Processing: {raw} => {final} \\t {pos}\")\n",
        "            dataset = self.preprocessing(file=\"/content/\"+str(raw))\n",
        "            # print(len(dataset))\n",
        "            with gzip.open('/content/' +str(final), 'wb') as f:\n",
        "                pickle.dump(dataset, f)\n",
        "                f.close()\n",
        "\n",
        "ds = PennTreeBankDataset()\n",
        "a = ds.export_files()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz8HGMzeGuUl"
      },
      "source": [
        "## 02. Prepare Dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhJy5weOGx7O"
      },
      "source": [
        "\n",
        "\n",
        "class PennTreeBankDictionary():\n",
        "\n",
        "    def load_corpus(self):\n",
        "        ds1 = PennTreeBankDataset()\n",
        "\n",
        "        print(\"preparing train/test/valid datasets\")\n",
        "        valid_ds = ds1.preprocessing(file=\"/content/22.auto.clean.txt\")\n",
        "        test_ds = ds1.preprocessing(file=\"/content/23.auto.clean.txt\")\n",
        "        train_ds = ds1.preprocessing(file=\"/content/02-21.10way.clean.txt\")\n",
        "        complete_ds = valid_ds + test_ds + train_ds\n",
        "        print(\"done\")\n",
        "\n",
        "        return complete_ds\n",
        "\n",
        "    def tokens_and_tags(self):\n",
        "\n",
        "        #Step 1: fetch dataset\n",
        "        dataset = self.load_corpus()\n",
        "        tokens, tags = [], []\n",
        "\n",
        "        #Step 2: split sentences and pos tags to two separate list\n",
        "        for sample in dataset:\n",
        "            tokens.append(sample[0])\n",
        "            tags.append(sample[1])\n",
        "\n",
        "        #Step 3: list of lists to a single flat list and take unique words and pos tags\n",
        "        all_tokens = [item for sublist in tokens for item in sublist]\n",
        "        all_tokens = list(set(all_tokens))\n",
        "        all_tags = [item for sublist in tags for item in sublist]\n",
        "        all_tags = list(set(all_tags))\n",
        "\n",
        "        return all_tokens, all_tags\n",
        "\n",
        "    def vocabulary(self):\n",
        "        \n",
        "        print(\"preparing look-up dictionaries\")\n",
        "        words_in_corpus, pos_tags_in_corpus = self.tokens_and_tags()\n",
        "        words_in_corpus.append('PADDING')\n",
        "        pos_tags_in_corpus.append('PADDING')\n",
        "        word_to_idx, pos_to_idx = {}, {}\n",
        "        idx_to_word, idx_to_pos = {}, {} # for reverse look-up\n",
        "\n",
        "        for idx, word in enumerate(words_in_corpus):\n",
        "            word_to_idx[word] = idx\n",
        "            idx_to_word[idx] = word\n",
        "\n",
        "        for idx,pos in enumerate(pos_tags_in_corpus):\n",
        "            pos_to_idx[pos] = idx\n",
        "            idx_to_pos[idx] = pos\n",
        "\n",
        "        print(\"done!\")\n",
        "        print(f\"Total words in the dictionary: {len(word_to_idx)} \\nTotal POS tags in the dictionary: {len(pos_to_idx)}\")\n",
        "        return word_to_idx, idx_to_word, pos_to_idx, idx_to_pos\n",
        "\n",
        "# ex = PennTreeBankDictionary()\n",
        "# a,b,c,d = ex.vocabulary()\n",
        "# print(list(a.items())[:10])\n",
        "# print(list(c.items())[:10])\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STpZk5qBG_Pv"
      },
      "source": [
        "## 03. Fetch dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDpb0fnhHH9d"
      },
      "source": [
        "from typing import List, Tuple\n",
        "# from prepareDictionary import PennTreeBankDictionary\n",
        "ds = PennTreeBankDictionary()\n",
        "# from prepareDataset import PennTreeBankDataset\n",
        "import torch\n",
        "import pickle, gzip\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class myDataset(Dataset):\n",
        "\n",
        "    def compose_dictionaries(self): \n",
        "\n",
        "        word_to_idx, idx_to_word, pos_to_idx, idx_to_pos = ds.vocabulary()\n",
        "        return word_to_idx, idx_to_word, pos_to_idx, idx_to_pos\n",
        "\n",
        "    def file_parser(self,filename) -> Tuple[List,List]:\n",
        "\n",
        "        filepath = \"/content/\"+filename+'.pklz'\n",
        "        samples_with_labels = []\n",
        "        file = gzip.open(filepath,'rb')\n",
        "        samples_with_labels = pickle.load(file)\n",
        "        \n",
        "        return samples_with_labels\n",
        "\n",
        "    def file_tensor(self, sentences_and_tags) -> Tuple[List, List]:\n",
        "\n",
        "        def token_pipeline(x):\n",
        "            if len(x) < 50:\n",
        "                for i in range(0,50-len(x)):\n",
        "                    x.append('PADDING')\n",
        "            return [self.word_to_idx[tok] for tok in x]\n",
        "\n",
        "        def pos_pipeline(x):\n",
        "            if len(x) < 50:\n",
        "                for i in range(0,50-len(x)):\n",
        "                    x.append('PADDING')\n",
        "            return [self.pos_to_idx[pos] for pos in x]\n",
        "\n",
        "        sent_to_idx, tags_to_idx = [], []\n",
        "        for sent_tag in sentences_and_tags:\n",
        "            if len(sent_tag[0]) >50:\n",
        "                continue\n",
        "            sent_to_idx.append(torch.tensor(token_pipeline(sent_tag[0])))\n",
        "            tags_to_idx.append(torch.tensor(pos_pipeline(sent_tag[1])))\n",
        "\n",
        "        return sent_to_idx, tags_to_idx\n",
        "\n",
        "    def __init__(self, raw_dataset=None):\n",
        "\n",
        "        print(\"STEP 01: Look-up Tables...\")\n",
        "        self.word_to_idx, self.idx_to_word, self.pos_to_idx, self.idx_to_pos = self.compose_dictionaries()\n",
        "        print(\"dictionaries ready!\")\n",
        "\n",
        "        print(\"STEP 02: Fetching the dataset...\")\n",
        "        self.samples_with_labels = self.file_parser(raw_dataset)\n",
        "        print(\"done!\")\n",
        "\n",
        "        print(\"STEP 03: Tokens and tags to Numbers...\")\n",
        "        self.samples_to_idx, self.labels_to_idx = self.file_tensor(self.samples_with_labels)\n",
        "\n",
        "    def __len__(self):    \n",
        "        return len(self.samples_to_idx)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.samples_to_idx[index], self.labels_to_idx[index]\n",
        "\n",
        "# validation_dataset = DataLoader(dataset=myDataset(\"PennTreeBankValid\")\n",
        "#                                         ,shuffle=False\n",
        "#                                         ,batch_size=16)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riAeVu68HQbS"
      },
      "source": [
        "## 04. Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpQEKGCzHWbj"
      },
      "source": [
        "# from typing import final\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "\"\"\"RNN Many-to-many multi-class classification neural network model structure definition\"\"\"\n",
        "\n",
        "class RNNPOSTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                embedding_dimension, \n",
        "                vocabulary_size,\n",
        "                hidden_dimension,\n",
        "                num_of_layers,\n",
        "                dropout,\n",
        "                output_dimension\n",
        "                ):\n",
        "        super(RNNPOSTagger, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
        "                                    embedding_dim=embedding_dimension,\n",
        "                                    padding_idx=45)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dimension,\n",
        "                            hidden_dimension,\n",
        "                            num_of_layers,\n",
        "                            dropout=dropout,\n",
        "                            batch_first=True)\n",
        "                            # bidirectional=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dimension, output_dimension)\n",
        "\n",
        "        # self.activation_fn = nn.Tanh()\n",
        "        self.activation_fn = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, sample):\n",
        "\n",
        "        # (1)- Embedding layer\n",
        "        embedded = self.embedding(sample)\n",
        "\n",
        "        #-------------------------------------------------------------------------\n",
        "\n",
        "        #(2)- LSTM layer 1\n",
        "        output, (hidden, cell) = self.lstm(embedded)       \n",
        "\n",
        "        #-------------------------------------------------------------------------\n",
        "\n",
        "        #concat the final forward and backward hidden state\n",
        "        hidden = torch.cat((hidden[-1,:,:], hidden[0,:,:]), dim = 1)\n",
        "\n",
        "\n",
        "        #(3)- LSTM to linear layer: Final set of tags\n",
        "        dense_output = self.fc(output)\n",
        "\n",
        "        #activation function\n",
        "        outputs=self.activation_fn(dense_output)\n",
        " \n",
        "        return outputs\n",
        "        # return dense_output"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGi7mJXGHfoW"
      },
      "source": [
        "## 05. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4UQIy6jHioe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0114c82-a7d1-43ba-c133-93afe33cc068"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# from fetchDataset import myDataset\n",
        "train_test_ds = myDataset\n",
        "# from prepareDictionary import PennTreeBankDictionary\n",
        "ds = PennTreeBankDictionary()\n",
        "import time\n",
        "import os\n",
        "# from model import RNNPOSTagger\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "################################### 01. Train/Test dataset  ########################################\n",
        "print(\"=\"*100)\n",
        "print(\"01. Preparing train/test datasets:\")\n",
        "\n",
        "train_dataset = DataLoader(dataset=train_test_ds(\"PennTreeBankTrain\"), batch_size=8, shuffle=True)\n",
        "test_dataset = DataLoader(dataset=train_test_ds(\"PennTreeBankTest\"), batch_size=8, shuffle=True)\n",
        "# validation_dataset = DataLoader(dataset=myDataset(\"PennTreeBankValid\"),batch_size=16,shuffle=True)\n",
        "print(\"datasets ready!\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "################################# 02.Model Parameters ####################################\n",
        "print(\"02. Loading Model Parameters:\")\n",
        "word_to_idx, idx_to_word, pos_to_idx, idx_to_pos = ds.vocabulary()\n",
        "\n",
        "# read this seq2seq model: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html --> for understanding embedding dimension and output dimension  \n",
        "VOCAB_SIZE = len(word_to_idx)+1\n",
        "EMBED_DIM = 100\n",
        "HIDDEN_DIM = 32\n",
        "NUM_LAYERS = 2\n",
        "NUM_OF_CLASSES = len(pos_to_idx)\n",
        "N_EPOCHS = 10\n",
        "LEARNING_RATE = 0.025#0.1\n",
        "BATCH_SIZE = 128#16\n",
        "\n",
        "print(f\"Size of vocabulary: {VOCAB_SIZE}\" + f\"\\tNumber of classes: {NUM_OF_CLASSES}\")\n",
        "##################################### 03. NN Model  ########################################\n",
        "\n",
        "print(\"Step 02. builing the model...\")\n",
        "model = RNNPOSTagger(embedding_dimension= EMBED_DIM,\n",
        "                    vocabulary_size=VOCAB_SIZE,\n",
        "                    hidden_dimension=HIDDEN_DIM,\n",
        "                    num_of_layers=NUM_LAYERS,\n",
        "                    dropout=0.1,\n",
        "                    output_dimension=NUM_OF_CLASSES)\n",
        "\n",
        "print(\"Done! here is our model:\")\n",
        "print(model)\n",
        "print(\"=\"*100)\n",
        "\n",
        "############################# 04. Optimizer and Loss  ####################################\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index=45)\n",
        "criterion = nn.NLLLoss(ignore_index=45)\n",
        "\n",
        "\n",
        "#define metric\n",
        "def train_accuracy(preds, y):\n",
        "    predicted_labels_dirty = preds.permute(0,2,1)\n",
        "    predicted_labels_final = torch.argmax(predicted_labels_dirty, dim=2).tolist()\n",
        "    actual_labels_final = y.tolist()\n",
        "    accuracy_of_all_lines = []\n",
        "    for predicted, actual in zip(predicted_labels_final, actual_labels_final):\n",
        "        counter = 0\n",
        "        # print(predicted)\n",
        "        # print(actual)\n",
        "        for pred,act in zip(predicted,actual):\n",
        "            if pred == act:\n",
        "                counter = counter+1\n",
        "        accuracy_of_this_line = counter/50\n",
        "        accuracy_of_all_lines.append(accuracy_of_this_line)\n",
        "    accuracy = sum(accuracy_of_all_lines)/len(predicted_labels_final)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def training_accuracy(preds, y):\n",
        "    \n",
        "    predsx = preds.permute(0,2,1) #reshape\n",
        "    predsx2 = torch.argmax(predsx, dim=2) #find POS index with max value for each token\n",
        "\n",
        "    for pred,act in zip(predsx2.tolist()[0],y.tolist()[0]):\n",
        "        counter = 0\n",
        "        if pred == act:\n",
        "            counter = counter+1\n",
        "        \n",
        "    # correct = (predsx2 == y)\n",
        "    # acc = correct.sum() / len(preds)\n",
        "    acc = counter/len(preds)\n",
        "    # print(type(acc))\n",
        "\n",
        "    return acc\n",
        "    \n",
        "#push to cuda if available\n",
        "# model = model.to(device)\n",
        "# criterion = criterion.to(device)\n",
        "\n",
        "############################## 05. NN Model Train Definition #############################\n",
        "\n",
        "def train(model, dataset, optimizer, criterion):\n",
        "\n",
        "    t = time.localtime()\n",
        "    start_time = time.strftime(\"%H:%M:%S\", t)\n",
        "    print(start_time)\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    epoch_dataset_length.append(len(dataset))\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for idx, (sample,label) in enumerate(dataset):\n",
        "       \n",
        "       current_samples = sample\n",
        "       current_labels = label\n",
        "\n",
        "       optimizer.zero_grad()\n",
        "\n",
        "       predicted_labels = model(current_samples).permute(0,2,1)\n",
        "      \n",
        "       loss = criterion(predicted_labels, current_labels)\n",
        "       accuracy = train_accuracy(predicted_labels, current_labels)\n",
        "\n",
        "       loss.backward()\n",
        "       optimizer.step()\n",
        "\n",
        "       epoch_loss += loss.item()\n",
        "       epoch_accuracy += accuracy\n",
        "\n",
        "    return epoch_loss/len(dataset), epoch_accuracy/sum(epoch_dataset_length)\n",
        "\n",
        "##########################################################################################\n",
        "################################ 06. NN Model Eval Definition ############################\n",
        "def evaluate(model, dataset, criterion):\n",
        "    \n",
        "    # start_time = time.time()\n",
        "    # print(start_time)\n",
        "\n",
        "    t = time.localtime()\n",
        "    start_time = time.strftime(\"%H:%M:%S\", t)\n",
        "    print(start_time)\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for idx, (sample,label) in enumerate(dataset):\n",
        "            current_samples = sample\n",
        "            current_labels = label\n",
        "\n",
        "            predicted_labels = model(current_samples).permute(0,2,1)\n",
        "\n",
        "            loss = criterion(predicted_labels, current_labels)\n",
        "            accuracy = train_accuracy(predicted_labels, current_labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_accuracy += accuracy\n",
        "\n",
        "    return epoch_loss/len(dataset), epoch_accuracy/len(dataset)\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "################################## 06. NN Model training #####################################\n",
        "#N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    print(f\"Epoch #: {epoch}\")\n",
        "    epoch_dataset_length = []\n",
        "    #train the model\n",
        "    train_loss, train_acc = train(model, train_dataset, optimizer, criterion)\n",
        "    \n",
        "    #evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, test_dataset, criterion)\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(\"-------------------------------------------------------------------\")\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "    print(\"-------------------------------------------------------------------\")\n",
        "\n",
        "# modelpath = \"notebooks\"\n",
        "# torch.save(model.state_dict(), os.path.join(modelpath, \"PennPOSmodel.pth\"))\n",
        "torch.save(model.state_dict(),\"/content/PennPOSmodel.pth\")\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====================================================================================================\n",
            "01. Preparing train/test datasets:\n",
            "STEP 01: Look-up Tables...\n",
            "preparing look-up dictionaries\n",
            "preparing train/test/valid datasets\n",
            "done\n",
            "done!\n",
            "Total words in the dictionary: 46349 \n",
            "Total POS tags in the dictionary: 46\n",
            "dictionaries ready!\n",
            "STEP 02: Fetching the dataset...\n",
            "done!\n",
            "STEP 03: Tokens and tags to Numbers...\n",
            "STEP 01: Look-up Tables...\n",
            "preparing look-up dictionaries\n",
            "preparing train/test/valid datasets\n",
            "done\n",
            "done!\n",
            "Total words in the dictionary: 46349 \n",
            "Total POS tags in the dictionary: 46\n",
            "dictionaries ready!\n",
            "STEP 02: Fetching the dataset...\n",
            "done!\n",
            "STEP 03: Tokens and tags to Numbers...\n",
            "datasets ready!\n",
            "====================================================================================================\n",
            "02. Loading Model Parameters:\n",
            "preparing look-up dictionaries\n",
            "preparing train/test/valid datasets\n",
            "done\n",
            "done!\n",
            "Total words in the dictionary: 46349 \n",
            "Total POS tags in the dictionary: 46\n",
            "Size of vocabulary: 46350\tNumber of classes: 46\n",
            "Step 02. builing the model...\n",
            "Done! here is our model:\n",
            "RNNPOSTagger(\n",
            "  (embedding): Embedding(46350, 100, padding_idx=45)\n",
            "  (lstm): LSTM(100, 32, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (fc): Linear(in_features=32, out_features=46, bias=True)\n",
            "  (activation_fn): LogSoftmax(dim=1)\n",
            ")\n",
            "====================================================================================================\n",
            "Epoch #: 0\n",
            "18:25:55\n",
            "18:26:47\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.436 | Train Acc: 66.48%\n",
            "\t Val. Loss: 1.176 |  Val. Acc: 76.95%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 1\n",
            "18:26:49\n",
            "18:27:43\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.135 | Train Acc: 77.21%\n",
            "\t Val. Loss: 1.134 |  Val. Acc: 78.19%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 2\n",
            "18:27:44\n",
            "18:28:37\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.074 | Train Acc: 70.13%\n",
            "\t Val. Loss: 1.115 |  Val. Acc: 44.42%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 3\n",
            "18:28:38\n",
            "18:29:33\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.042 | Train Acc: 63.22%\n",
            "\t Val. Loss: 1.129 |  Val. Acc: 64.81%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 4\n",
            "18:29:34\n",
            "18:30:30\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.040 | Train Acc: 69.52%\n",
            "\t Val. Loss: 1.122 |  Val. Acc: 78.34%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 5\n",
            "18:30:31\n",
            "18:31:29\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.026 | Train Acc: 74.12%\n",
            "\t Val. Loss: 1.124 |  Val. Acc: 76.79%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 6\n",
            "18:31:31\n",
            "18:32:31\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.017 | Train Acc: 75.44%\n",
            "\t Val. Loss: 1.119 |  Val. Acc: 80.50%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 7\n",
            "18:32:32\n",
            "18:33:33\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.013 | Train Acc: 74.39%\n",
            "\t Val. Loss: 1.121 |  Val. Acc: 80.34%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 8\n",
            "18:33:34\n",
            "18:34:36\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.009 | Train Acc: 73.66%\n",
            "\t Val. Loss: 1.143 |  Val. Acc: 80.27%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 9\n",
            "18:34:38\n",
            "18:35:40\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.016 | Train Acc: 69.64%\n",
            "\t Val. Loss: 1.136 |  Val. Acc: 77.81%\n",
            "-------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QDhX72YXSSz",
        "outputId": "0f041e79-a44a-4f58-8bca-dc7c055bf0aa"
      },
      "source": [
        "############################################################################################\n",
        "################################## 07. Model Predictions #####################################\n",
        "from typing import Tuple, List\n",
        "# from model import RNNPOSTagger\n",
        "# from dataset import WSJDataset, vocabulary\n",
        "# from fetchDataset import myDataset\n",
        "# from prepareDictionary import PennTreeBankDictionary\n",
        "ds2 = PennTreeBankDictionary()\n",
        "valid_ds = myDataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pickle,gzip\n",
        "\n",
        "############################### 01. Look-up dictionaries ####################################\n",
        "word_to_idx, idx_to_word, pos_to_idx, idx_to_pos = ds2.vocabulary()\n",
        "\n",
        "validation_dataset = DataLoader(dataset=valid_ds(\"PennTreeBankValid\"),batch_size=16,shuffle=False)\n",
        "\n",
        "# read this seq2seq model: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html --> for understanding embedding dimension and output dimension  \n",
        "# VOCAB_SIZE = len(word_to_idx)+1\n",
        "# EMBED_DIM = 100\n",
        "# HIDDEN_DIM = 64\n",
        "# NUM_LAYERS = 1\n",
        "# NUM_OF_CLASSES = len(pos_to_idx)+1\n",
        "# N_EPOCHS = 5\n",
        "# LEARNING_RATE = 0.005\n",
        "# BATCH_SIZE = 64\n",
        "\n",
        "VOCAB_SIZE = len(word_to_idx)+1\n",
        "EMBED_DIM = 100\n",
        "HIDDEN_DIM = 32\n",
        "NUM_LAYERS = 2\n",
        "NUM_OF_CLASSES = len(pos_to_idx)\n",
        "N_EPOCHS = 20\n",
        "LEARNING_RATE = 0.025\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "print(f\"Our vocab size to the model is therefore: {VOCAB_SIZE}\")\n",
        "################################### 02. NN Model  ########################################\n",
        "\n",
        "print(\"Step 02. builing the model...\")\n",
        "model = RNNPOSTagger(embedding_dimension= EMBED_DIM,\n",
        "                            vocabulary_size=VOCAB_SIZE,\n",
        "                            hidden_dimension=HIDDEN_DIM,\n",
        "                            num_of_layers=NUM_LAYERS,\n",
        "                            dropout=0.1,\n",
        "                            output_dimension=NUM_OF_CLASSES)\n",
        "print(\"----------------------------------------------------------------\")\n",
        "print(\"Done! here is our model:\")\n",
        "print(model)\n",
        "print(\"----------------------------------------------------------------\")\n",
        "\n",
        "################################## 03. load trained model ###############################\n",
        "model.load_state_dict(torch.load(\"/content/PennPOSmodel.pth\"))\n",
        "model.eval()\n",
        "\n",
        "################################## 03. Predictions ###############################\n",
        "print(\"Lets make predictions\")\n",
        "\n",
        "def token_pipeline(x):\n",
        "    \n",
        "    if len(x) < 50:\n",
        "        for i in range(1,60-len(x)):\n",
        "            x.append('PADDING')\n",
        "    return [word_to_idx[tok] for tok in x]\n",
        "\n",
        "def token_reverse_pipeline(x):\n",
        "    return [idx_to_word[idx] for idx in x]\n",
        "\n",
        "def pos_reverse_pipeline(x):\n",
        "    return [idx_to_pos[idx] for idx in x]\n",
        "\n",
        "def pos_pipeline(x):\n",
        "    return [pos_to_idx[pos] for pos in x]\n",
        "##############################################################################################\n",
        "def predict_full_validation_dataset(example_sentence) -> Tuple[List, List]:\n",
        "    sentence_to_tensor = example_sentence.unsqueeze(1).T\n",
        "    with torch.no_grad():\n",
        "        output = model(sentence_to_tensor)\n",
        "        predicted_output = torch.argmax(output, dim=2)\n",
        "        example_predicted_labels = pos_reverse_pipeline(predicted_output.tolist()[0])\n",
        "        example_sentence_words = token_reverse_pipeline(sentence_to_tensor.tolist()[0])\n",
        "\n",
        "    # return example_predicted_labels\n",
        "    return example_sentence_words,example_predicted_labels\n",
        "###############################################################################################\n",
        "def predict_example(example_sentence, example_actual_labels):\n",
        "\n",
        "    \n",
        "\n",
        "    # preprocessing:-\n",
        "    sentence_to_token = token_pipeline(example_sentence)\n",
        "    sentence_to_tensor = torch.tensor(sentence_to_token).unsqueeze(1).T\n",
        "\n",
        "    # predicted labels:-\n",
        "    with torch.no_grad():\n",
        "        output = model(sentence_to_tensor)\n",
        "        predicted_output = torch.argmax(output, dim=2)\n",
        "        print(predicted_output)\n",
        "        #-------------\n",
        "        print(pos_pipeline(example_actual_labels))\n",
        "        # print(predicted_output.tolist()[0][:-1])\n",
        "        print(predicted_output.tolist()[0][:len(example_actual_labels)])\n",
        "\n",
        "        example_predicted_labels = pos_reverse_pipeline(predicted_output.tolist()[0])\n",
        "        print(\"-\"*100)\n",
        "        print(f\"Actual lables:- \\n{example_actual_labels}\")\n",
        "        print(f\"Predicted lables:- \\n{example_predicted_labels[:len(example_actual_labels)]}\")\n",
        "        print(\"-\"*100)\n",
        "    # return example_predicted_labels[:len(example_actual_labels)]\n",
        "###################################################################################################\n",
        "example = [['This', 'time', ',', 'the', 'firms', 'were', 'ready', '.'],\n",
        "            ['We', \"'re\", 'about', 'to', 'see', 'if', 'advertising', 'works', '.']]\n",
        "example_labels = [['DT', 'NN', ',', 'DT', 'NNS', 'VBD', 'JJ', '.'],\n",
        "                ['PRP', 'VBP', 'IN', 'TO', 'VB', 'IN', 'NN', 'VBZ', '.']]\n",
        "\n",
        "predict_example(example_sentence=example[0],example_actual_labels=example_labels[0])\n",
        "print(\"EXAMPLE 2\")\n",
        "predict_example(example_sentence=example[1],example_actual_labels=example_labels[1])\n",
        "\n",
        "##################################################################################################\n",
        "# print(\"Composing the result of first nn network to POS tag the dataset:\")\n",
        "# all_results = []\n",
        "# for idx, (sample, label) in enumerate(validation_dataset):\n",
        "#     for item in sample:\n",
        "#         all_results.append(predict_full_validation_dataset(item))\n",
        "# # print(all_results[:2])\n",
        "# with gzip.open('C:/Users/rahin/projects/WSJ-POS-tagger/data/interim/validation_dataset_pos_tagged.pklz', 'wb') as f:\n",
        "#     pickle.dump(all_results, f)\n",
        "#     f.close()\n",
        "# print(\"done!\")\n",
        "#########################################################################################    \n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preparing look-up dictionaries\n",
            "preparing train/test/valid datasets\n",
            "done\n",
            "done!\n",
            "Total words in the dictionary: 46349 \n",
            "Total POS tags in the dictionary: 46\n",
            "STEP 01: Look-up Tables...\n",
            "preparing look-up dictionaries\n",
            "preparing train/test/valid datasets\n",
            "done\n",
            "done!\n",
            "Total words in the dictionary: 46349 \n",
            "Total POS tags in the dictionary: 46\n",
            "dictionaries ready!\n",
            "STEP 02: Fetching the dataset...\n",
            "done!\n",
            "STEP 03: Tokens and tags to Numbers...\n",
            "Our vocab size to the model is therefore: 46350\n",
            "Step 02. builing the model...\n",
            "----------------------------------------------------------------\n",
            "Done! here is our model:\n",
            "RNNPOSTagger(\n",
            "  (embedding): Embedding(46350, 100, padding_idx=45)\n",
            "  (lstm): LSTM(100, 32, num_layers=2, batch_first=True, dropout=0.1)\n",
            "  (fc): Linear(in_features=32, out_features=46, bias=True)\n",
            "  (activation_fn): LogSoftmax(dim=1)\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "Lets make predictions\n",
            "tensor([[25, 29, 23,  3, 30,  5, 17, 37, 45, 27, 27, 27, 27, 27, 45, 45, 45, 45,\n",
            "         45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n",
            "         45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n",
            "         45, 45, 45, 45, 45]])\n",
            "[3, 29, 23, 3, 41, 5, 17, 37]\n",
            "[25, 29, 23, 3, 30, 5, 17, 37]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Actual lables:- \n",
            "['DT', 'NN', ',', 'DT', 'NNS', 'VBD', 'JJ', '.']\n",
            "Predicted lables:- \n",
            "['SYM', 'NN', ',', 'DT', 'UH', 'VBD', 'JJ', '.']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "EXAMPLE 2\n",
            "tensor([[28, 24, 33, 21, 39, 35, 29,  4, 37, 27, 27, 27, 27, 27, 45, 45, 45, 45,\n",
            "         45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n",
            "         45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n",
            "         45, 45, 45, 45, 45]])\n",
            "[22, 38, 44, 21, 39, 44, 29, 4, 37]\n",
            "[28, 24, 33, 21, 39, 35, 29, 4, 37]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Actual lables:- \n",
            "['PRP', 'VBP', 'IN', 'TO', 'VB', 'IN', 'NN', 'VBZ', '.']\n",
            "Predicted lables:- \n",
            "['EX', '-RRB-', 'RP', 'TO', 'VB', 'PRP$', 'NN', 'VBZ', '.']\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqENkahSYr0i"
      },
      "source": [
        "# result comparison\n",
        "all_actual_labels, all_predicted_labels = [],[] \n",
        "\n",
        "def pos_reverse_pipeline(x):\n",
        "    return [idx_to_pos[idx] for idx in x]\n",
        "\n",
        "for idx, (sample,label) in enumerate(validation_dataset):\n",
        "  for sam in sample:\n",
        "    all_predicted_labels.append(predict_full_validation_dataset(sam)[1])\n",
        "  for lab in label:\n",
        "    all_actual_labels.append(pos_reverse_pipeline(lab.tolist()))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D80XOmrHZ00H",
        "outputId": "1150caef-a983-424f-ba11-577dac70c503"
      },
      "source": [
        "def train_accuracy(preds, y):\n",
        "\n",
        "    # print(len(preds)) #i get 10 samples\n",
        "    accuracy_of_all_lines = []\n",
        "\n",
        "    \n",
        "    \n",
        "    for pred,act in zip(preds,y):\n",
        "        \n",
        "        counter = 0\n",
        "        \n",
        "        for itemx,itemj in zip(pred,act):\n",
        "          \n",
        "          \n",
        "          if itemx == itemj:\n",
        "              counter = counter+1\n",
        "        accuracy_of_this_line = counter/50\n",
        "\n",
        "        accuracy_of_all_lines.append(accuracy_of_this_line)\n",
        "    \n",
        "    # print(accuracy_of_all_lines)\n",
        "    acc = sum(accuracy_of_all_lines)/len(preds)\n",
        "\n",
        "    return acc*100\n",
        "train_accuracy(all_predicted_labels,all_actual_labels)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77.45997610513705"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2hV3wCiSuIJ"
      },
      "source": [
        "# (02) BIOES Tagging Neural Network\n",
        "\n",
        "## 1. ConLL Dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLhRBNx2LQ3R",
        "outputId": "a3040b97-8891-4027-ff25-ba3611c7df20"
      },
      "source": [
        "# Dictionary Construction:\n",
        "# (1) Word look-up table W\n",
        "# (2) POS look-up table P\n",
        "# (3) BIOES look-up table T\n",
        "\n",
        "import pickle\n",
        "\n",
        "def read_file(input_file: str):\n",
        "\n",
        "    current_file = input_file.split('\\\\')[-1]\n",
        "    print(f\"Currently processing the file: {current_file}\")    \n",
        "    file_content = open(input_file, mode='r')\n",
        "\n",
        "    return file_content.readlines()\n",
        "\n",
        "def find_tokens(input_dataset: str):\n",
        "\n",
        "    all_tokens = []\n",
        "    all_pos_tags = []\n",
        "    all_ner_tags = []\n",
        "\n",
        "    for line in input_dataset:\n",
        "        if len(line) != 1:\n",
        "            list_of_entities = line.split(' ')\n",
        "\n",
        "        #print(list_of_entities)\n",
        "        if len(list_of_entities) != 0:\n",
        "            all_tokens.append(list_of_entities[0])\n",
        "            all_pos_tags.append(list_of_entities[1])\n",
        "            all_ner_tags.append(list_of_entities[2])\n",
        "    \n",
        "    return all_tokens, all_pos_tags, all_ner_tags\n",
        "\n",
        "# Step 1: get file contents\n",
        "train_dataset = read_file(\"/content/ConLL2003-bioes-train.txt\")\n",
        "test_dataset = read_file(\"/content/ConLL2003-bioes-test.txt\")\n",
        "valid_dataset = read_file(\"/content/ConLL2003-bioes-valid.txt\")\n",
        "\n",
        "# Step 2: get tokens, pos and bioes tags in all 3 files\n",
        "v1, p1, t1 =  find_tokens(train_dataset)\n",
        "v2, p2, t2 =  find_tokens(test_dataset)\n",
        "v3, p3, t3 =  find_tokens(valid_dataset)\n",
        "tokens_in_all_files = v1 + v2 + v3\n",
        "pos_tags_in_all_files = p1 + p2 + p3\n",
        "bioes_tags_in_all_files =  t1 + t2 + t3\n",
        "\n",
        "# Step 3: Remove duplicates\n",
        "all_unique_tokens = list(set(tokens_in_all_files))\n",
        "all_unique_pos_tags = list(set(pos_tags_in_all_files))\n",
        "all_unique_bioes_tags = list(set(bioes_tags_in_all_files))\n",
        "\n",
        "# Step 4: Vocabulary, POS tags and BIOES tags dictionaries\n",
        "vocabulary = {}\n",
        "pos_tags = {}\n",
        "BIOES_tags = {}\n",
        "for idx, token in enumerate(all_unique_tokens):\n",
        "    vocabulary[token] = idx\n",
        "for idx, pos in enumerate(all_unique_pos_tags):\n",
        "    pos_tags[pos] = idx\n",
        "for idx, bioes in enumerate(all_unique_bioes_tags):\n",
        "    BIOES_tags[bioes] = idx  \n",
        "\n",
        "vocabulary['PADDING'] = len(vocabulary)+1\n",
        "pos_tags['PADDING'] = len(pos_tags)+1\n",
        "BIOES_tags['PADDING'] = len(BIOES_tags)+1\n",
        "# Step 5: Export dictionaries\n",
        "vocab_to_file = open(\"/content/ConLL2003_vocabulary.pkl\",\"wb\")\n",
        "pickle.dump(vocabulary, vocab_to_file)\n",
        "vocab_to_file.close()\n",
        "\n",
        "tags_to_file = open(\"/content/ConLL2003_pos_tags.pkl\",\"wb\")\n",
        "pickle.dump(pos_tags, tags_to_file)\n",
        "tags_to_file.close()\n",
        "\n",
        "bioes_to_file = open(\"/content/ConLL2003_BIOES_tags.pkl\",\"wb\")\n",
        "pickle.dump(BIOES_tags, bioes_to_file)\n",
        "bioes_to_file.close()\n",
        "\n",
        "print(\"Done! All 3 dictionaries are now available for you...\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currently processing the file: /content/ConLL2003-bioes-train.txt\n",
            "Currently processing the file: /content/ConLL2003-bioes-test.txt\n",
            "Currently processing the file: /content/ConLL2003-bioes-valid.txt\n",
            "Done! All 3 dictionaries are now available for you...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdvEOEsKcosY"
      },
      "source": [
        "# Update Key names to match Penn Tree bank POS comback later\n",
        "# dictionary[new_key] = dictionary.pop(old_key)\n",
        "# pos_tags['``'] = pos_tags.pop('\"')\n",
        "pos_tags['-LRB-'] = pos_tags.pop('(')\n",
        "pos_tags['-RRB-'] = pos_tags.pop(')')\n",
        "pos_tags['#'] = pos_tags.pop('-X-')\n",
        "# pos_tags['``'] = pos_tags.pop('\"')\n",
        "# pos_tags.keys()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjb5UJWKbF9r"
      },
      "source": [
        "## 02. ConLL Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlInF_mGbFM-"
      },
      "source": [
        "# PyTorch dataset preprocessing, (sample, label) iterator\n",
        "\n",
        "import pickle\n",
        "from typing import List, Tuple\n",
        "# from torch._C import dtype\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class SlidingWindowDataset(Dataset):\n",
        "\n",
        "    def file_open(self, filepath: str, ftype: str):\n",
        "\n",
        "        \"\"\"file operations: open, read, return file content and close\"\"\"\n",
        "        \n",
        "        if ftype == \"pickle\":                           #dictionary\n",
        "\n",
        "            file = open(filepath, \"rb\")\n",
        "            content = pickle.load(file)\n",
        "            file.close()\n",
        "        else:                                           #dataset\n",
        "            filename = filepath.split('/')[-1]\n",
        "            print(f\"loading dataset: {filename}\")\n",
        "            content = open(filepath, mode='r').read()\n",
        "\n",
        "        return content \n",
        "\n",
        "    def load_dictionaries(self):\n",
        "\n",
        "        \"\"\"Look-up tables: Word (W), POS tags(P) and BIOES tags(T)\"\"\"\n",
        "\n",
        "        print(\"loading dictionaries...\")\n",
        "\n",
        "        list_of_dicts = [\"/content/ConLL2003_vocabulary.pkl\",\n",
        "                        \"/content/ConLL2003_pos_tags.pkl\",\n",
        "                        \"/content/ConLL2003_BIOES_tags.pkl\"]\n",
        "        \n",
        "        vocabulary = self.file_open(filepath= list_of_dicts[0], ftype= \"pickle\")\n",
        "        pos_tags = self.file_open(filepath= list_of_dicts[1], ftype= \"pickle\")\n",
        "        bioes_tags = self.file_open(filepath= list_of_dicts[2], ftype= \"pickle\")\n",
        "\n",
        "        # add padding vector to each dict\n",
        "        # vocabulary['PADDING'] = len(vocabulary)+1\n",
        "        # pos_tags['PADDING'] = len(pos_tags)+1\n",
        "        # bioes_tags['PADDING'] = len(bioes_tags)+1\n",
        "\n",
        "        print(\"done\")\n",
        "\n",
        "        return vocabulary, pos_tags, bioes_tags\n",
        "\n",
        "    def file_processing(self):\n",
        "\n",
        "        \"\"\"create tuple ((word, pos, bioes)) from each sample\"\"\"\n",
        "\n",
        "        print(\"Performing dataset pre-processing activities...\")\n",
        "\n",
        "        all_samples, all_pos_tags, all_bioes_tags, all_samples_tuples = [], [], [], []\n",
        "\n",
        "        #max_sentence_length = []\n",
        "        dataset = self.file_open(filepath = self.mydataset, ftype=\"dataset\")\n",
        "        #dataset = self.file_open(filepath= r\"data\\raw\\ConLL2003-bioes-valid.txt\", ftype=\"dataset\")\n",
        "        dataset = dataset.split(\". . O O\") #break by sentences\n",
        "\n",
        "        for sentence in dataset: # each sentence\n",
        "\n",
        "            sentence = sentence.split('\\n') # each line\n",
        "            all_words, all_pos, all_bioes = [], [], [] # refresh for each sentence\n",
        "            for words in sentence:\n",
        "\n",
        "                if len(words.split(' ')) > 1:\n",
        "                    all_words.append(words.split(' ')[0])\n",
        "                    all_pos.append(words.split(' ')[1])\n",
        "                    all_bioes.append(words.split(' ')[2])\n",
        "\n",
        "            #max_sentence_length.append(len(all_words))\n",
        "\n",
        "            all_samples.append(all_words)\n",
        "            all_pos_tags.append(all_pos)\n",
        "            all_bioes_tags.append(all_bioes)\n",
        "\n",
        "        #padding logic max: 1178\n",
        "\n",
        "\n",
        "            all_samples_tuples.append(list(zip(all_samples, all_pos_tags, all_bioes_tags))) # collate the information\n",
        "        \n",
        "        print(\"done\")\n",
        "            \n",
        "        return all_samples_tuples\n",
        "\n",
        "    def file_parser(self, processed_samples: str) -> Tuple[List, List]:\n",
        "\n",
        "        print(\"Parsing the dataset now...\")\n",
        "\n",
        "        def sample_word_pipeline(x):                        # word to idx\n",
        "            return [self.vocabulary[tok] for tok in x]\n",
        "        \n",
        "        def sample_pos_pipeline(x):                         # pos to idx\n",
        "            return [self.pos_tags[pos] for pos in x]\n",
        "\n",
        "        def label_pipeline(x):                              # BIOES to idx\n",
        "            return [self.bioes_tags[bioes] for bioes in x]\n",
        "\n",
        "        def sliding_window(x):                              #opt #1: idx sliding window\n",
        "            window1, window2 = [], []\n",
        "            start, stop = 0, 5\n",
        "            for i in range(0,46):\n",
        "                window1.append(x[start:stop])\n",
        "                start +=1\n",
        "                stop +=1\n",
        "\n",
        "            for window in window1:\n",
        "                for win in window:\n",
        "                    window2.append(win)\n",
        "            return window2\n",
        "        \n",
        "        # sliding window (thanks to https://diegslva.github.io/2017-05-02-first-post/)\n",
        "        def pytorch_rolling_window(x, window_size, step_size=1):    #opt#2: Tensor idx sliding window\n",
        "\n",
        "            return x.unfold(0,window_size,step_size)\n",
        "\n",
        "        samples, labels = [], []\n",
        "\n",
        "        print(\"converting tokens to indices to tensors\")\n",
        "\n",
        "        for idx, sample in enumerate(processed_samples):\n",
        "\n",
        "            current_sample = list(sample)[idx][0]\n",
        "\n",
        "            if len(current_sample) > 50:            #excluding samples with token size >50 for simplicity\n",
        "                continue\n",
        "            else:\n",
        "                # padding logic\n",
        "                for padding in range(50-len(current_sample)):\n",
        "                    current_sample.append('PADDING')\n",
        "\n",
        "            current_sample_to_idx = sample_word_pipeline(current_sample)  #padded sentence tokens to idx\n",
        "            current_sample = torch.tensor(current_sample_to_idx, dtype=torch.int64) #idx to tensor\n",
        "            # current_sample_idx_to_windows = sliding_window(current_sample_to_idx)\n",
        "            # current_sample = torch.tensor(current_sample_idx_to_windows, dtype=torch.int64)\n",
        "\n",
        "            current_pos = sample[idx][1]\n",
        "\n",
        "            if len(current_pos) > 50:\n",
        "                continue\n",
        "            else:\n",
        "                # padding logic\n",
        "                for padding in range(50-len(current_pos)):\n",
        "                    current_pos.append('PADDING')\n",
        "\n",
        "            current_pos_to_idx = sample_pos_pipeline(current_pos)\n",
        "            current_pos = torch.tensor(current_pos_to_idx, dtype=torch.int64)\n",
        "            # current_pos_idx_to_windows = sliding_window(current_pos_to_idx)\n",
        "            # current_pos = torch.tensor(current_pos_idx_to_windows, dtype=torch.int64)\n",
        "\n",
        "            current_bioes = sample[idx][2]\n",
        "\n",
        "            if len(current_bioes) > 50:\n",
        "                continue\n",
        "            else:\n",
        "                # padding logic\n",
        "                for padding in range(50-len(current_bioes)):\n",
        "                    current_bioes.append('PADDING')\n",
        "\n",
        "            current_bioes_to_idx = label_pipeline(current_bioes)\n",
        "            current_bioes = torch.tensor(current_bioes_to_idx, dtype=torch.int64)\n",
        "            # current_bioes_idx_to_windows = sliding_window(current_bioes_to_idx)\n",
        "            # current_bioes = torch.tensor(current_bioes_idx_to_windows, dtype=torch.int64)  \n",
        "\n",
        "            \"\"\" Simple Test code-piece to assert that length of samples (word+pos) matches length of the labels (bioes)\"\"\"\n",
        "            if len(current_sample+current_pos) != len(current_bioes):\n",
        "                print(\"Attention! Lengths don't match here:\")\n",
        "                print(current_sample)\n",
        "                print(current_pos)\n",
        "                print(current_bioes)\n",
        "                exit\n",
        "\n",
        "            samples.append(current_sample+current_pos)\n",
        "            labels.append(current_bioes)\n",
        "\n",
        "        print(\"done\")\n",
        "\n",
        "        if(len(samples) == len(labels)):\n",
        "            print(\"Length matches! Hurray!\")\n",
        "        else:\n",
        "            print(\"Oops, something went wrong, check the dimension of samples and labels\")\n",
        "\n",
        "        return samples, labels\n",
        "\n",
        "############################################################################################################\n",
        "\n",
        "    def __init__(self, myDataset=None):\n",
        "\n",
        "        self.mydataset = myDataset\n",
        "        self.vocabulary, self.pos_tags, self.bioes_tags = self.load_dictionaries()\n",
        "        self.all_samples = self.file_processing()\n",
        "        self.samples, self.labels = self.file_parser(self.all_samples)\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx) :\n",
        "\n",
        "        return self.samples[idx], self.labels[idx]\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY7pyZMxbcC4"
      },
      "source": [
        "## 03. Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0ulgkhZbbvM"
      },
      "source": [
        "# from typing import final\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "\"\"\"RNN Many-to-many multi-class classification neural network model structure definition\"\"\"\n",
        "\n",
        "class RNNBIOESTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                embedding_dimension, \n",
        "                vocabulary_size,\n",
        "                hidden_dimension,\n",
        "                num_of_layers,\n",
        "                dropout,\n",
        "                output_dimension\n",
        "                ):\n",
        "        super(RNNBIOESTagger, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
        "                                    embedding_dim=embedding_dimension)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dimension,\n",
        "                            hidden_dimension,\n",
        "                            num_of_layers,\n",
        "                            dropout=dropout,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dimension*2, output_dimension)#230)#\n",
        "        # self.fc = nn.Linear(hidden_dimension, output_dimension)\n",
        "\n",
        "        self.activation_fn = nn.Tanh()\n",
        "\n",
        "\n",
        "    def forward(self, sample):\n",
        "\n",
        "        # (1)- Embedding layer\n",
        "        embedded = self.embedding(sample)\n",
        "\n",
        "        #-------------------------------------------------------------------------\n",
        "\n",
        "        #(2)- LSTM layer 1\n",
        "        output, (hidden, cell) = self.lstm(embedded)       \n",
        "\n",
        "        #-------------------------------------------------------------------------\n",
        "\n",
        "        #concat the final forward and backward hidden state\n",
        "        #hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "        hidden = torch.cat((hidden[-1,:,:], hidden[0,:,:]), dim = 1)\n",
        "\n",
        "\n",
        "        #(3)- LSTM to linear layer: Final set of tags\n",
        "        dense_output = self.fc(output)\n",
        "        # print(f\"LSTM to Linear layer output dimension {dense_output.size()}\")\n",
        "        \n",
        "\n",
        "        #activation function\n",
        "        outputs=self.activation_fn(dense_output)\n",
        " \n",
        "        return outputs\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Zwi5zFb1K4"
      },
      "source": [
        "## 04. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1zk0UpfYb3lf",
        "outputId": "270b5809-f474-4524-f225-3e8fce76f9a0"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "# from datasetConLL2003 import SlidingWindowDataset\n",
        "# from modelConLL2003 import RNNBIOESTagger\n",
        "\n",
        "print(\"Step 1: loading Train/Test/Validation datasets...\")\n",
        "print(\"*\"*100)     \n",
        "validation_dataset = DataLoader(dataset=SlidingWindowDataset(\"/content/ConLL2003-bioes-valid.txt\"),\n",
        "                                batch_size=64,\n",
        "                                shuffle=True)\n",
        "print(\"*\"*100)                                \n",
        "test_dataset = DataLoader(dataset=SlidingWindowDataset(\"/content/ConLL2003-bioes-test.txt\"),\n",
        "                                batch_size=64,\n",
        "                                shuffle=True)                                \n",
        "print(\"*\"*100)                                 \n",
        "train_dataset = DataLoader(dataset=SlidingWindowDataset(\"/content/ConLL2003-bioes-train.txt\"),\n",
        "                                batch_size=64,\n",
        "                                shuffle=True)\n",
        "print(\"*\"*100) \n",
        "print(\"All datasets successfully loaded!\")\n",
        "\n",
        "\n",
        "##########################################################################################\n",
        "idx_to_BIOES = {}\n",
        "ds = SlidingWindowDataset(\"/content/ConLL2003-bioes-valid.txt\")\n",
        "x1, x2, y = ds.load_dictionaries()\n",
        "for key, value in y.items():\n",
        "    idx_to_BIOES[value] = key\n",
        "print(f\"Length of vocabulary is: {len(x1)} and Length of POS table is: {len(x2)}\")\n",
        "print(f\"Length of Target look-up table is: {len(y)}\")\n",
        "\n",
        "################################# 01.Model Parameters ####################################\n",
        "\n",
        "# read this seq2seq model: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html --> for understanding embedding dimension and output dimension  \n",
        "VOCAB_SIZE = len(x1)+len(x2)+2\n",
        "EMBED_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "NUM_LAYERS = 2\n",
        "NUM_OF_CLASSES = len(y)+1\n",
        "N_EPOCHS = 10\n",
        "LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "print(f\"Our vocab size to the model is therefore: {VOCAB_SIZE}\")\n",
        "\n",
        "################################### 02. NN Model  ########################################\n",
        "\n",
        "print(\"Step 02. builing the model...\")\n",
        "model = RNNBIOESTagger(embedding_dimension= EMBED_DIM,\n",
        "                            vocabulary_size=VOCAB_SIZE,\n",
        "                            hidden_dimension=HIDDEN_DIM,\n",
        "                            num_of_layers=NUM_LAYERS,\n",
        "                            dropout=0.2,\n",
        "                            output_dimension=NUM_OF_CLASSES)\n",
        "print(\"----------------------------------------------------------------\")\n",
        "print(\"Done! here is our model:\")\n",
        "print(model)\n",
        "print(\"----------------------------------------------------------------\")\n",
        "\n",
        "############################# 03. Optimizer and Loss  ####################################\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "#optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_accuracy(preds, y):\n",
        "    predicted_labels_dirty = preds.permute(0,2,1)\n",
        "    predicted_labels_final = torch.argmax(predicted_labels_dirty, dim=2).tolist()\n",
        "    actual_labels_final = y.tolist()\n",
        "    accuracy_of_all_lines = []\n",
        "    for predicted, actual in zip(predicted_labels_final, actual_labels_final):\n",
        "        counter = 0\n",
        "        for pred,act in zip(predicted,actual):\n",
        "            if pred == act:\n",
        "                counter = counter+1\n",
        "        accuracy_of_this_line = counter/50\n",
        "        accuracy_of_all_lines.append(accuracy_of_this_line)\n",
        "    accuracy = sum(accuracy_of_all_lines)/len(predicted_labels_final)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "#define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    \n",
        "    predsx = preds.permute(0,2,1) #reshape\n",
        "    predsx2 = torch.argmax(predsx, dim=2) #find BIOES index with max value for each token\n",
        "    correct = (predsx2 == y)\n",
        "    acc = correct.sum() / len(preds)\n",
        "    # print(type(acc))\n",
        "\n",
        "    return acc\n",
        "    \n",
        "#push to cuda if available\n",
        "# model = model.to(device)\n",
        "# criterion = criterion.to(device)\n",
        "\n",
        "##########################################################################################\n",
        "############################## 04. NN Model Train Definition #############################\n",
        "\n",
        "def train(model, dataset, optimizer, criterion):\n",
        "    \n",
        "    t = time.localtime()\n",
        "    start_time = time.strftime(\"%H:%M:%S\", t)\n",
        "    print(start_time)\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    epoch_dataset_length_total = 0\n",
        "    epoch_dataset_length.append(len(dataset))\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for idx, (sample, label) in enumerate(dataset):\n",
        "       \n",
        "       current_samples = sample\n",
        "       current_labels = label\n",
        "\n",
        "       optimizer.zero_grad()\n",
        "\n",
        "       predicted_labels = model(current_samples).permute(0,2,1)\n",
        "       \n",
        " \n",
        "       loss = criterion(predicted_labels, current_labels)\n",
        "       accuracy = train_accuracy(predicted_labels, current_labels)\n",
        "\n",
        "       loss.backward()\n",
        "       optimizer.step()\n",
        "\n",
        "       epoch_loss += loss.item()\n",
        "       epoch_accuracy += accuracy\n",
        "\n",
        "    #    epoch_accuracy =0\n",
        "\n",
        "    return epoch_loss/len(dataset), epoch_accuracy/sum(epoch_dataset_length)\n",
        "\n",
        "##########################################################################################\n",
        "################################ 05. NN Model Eval Definition ############################\n",
        "def evaluate(model, dataset, criterion):\n",
        "    \n",
        "    t = time.localtime()\n",
        "    start_time = time.strftime(\"%H:%M:%S\", t)\n",
        "    print(start_time)\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for idx, (sample, label) in enumerate(dataset):\n",
        "            current_samples = sample\n",
        "            current_labels = label\n",
        "\n",
        "            predicted_labels = model(current_samples).permute(0,2,1)\n",
        "\n",
        "\n",
        "            loss = criterion(predicted_labels, current_labels)\n",
        "            accuracy = train_accuracy(predicted_labels, current_labels)\n",
        "            #epoch_accuracy = 0\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_accuracy += accuracy\n",
        "\n",
        "    return epoch_loss/len(dataset), epoch_accuracy/len(dataset)\n",
        "\n",
        "############################################################################################\n",
        "################################## 06. NN Model training #####################################\n",
        "#N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    print(f\"Epoch #: {epoch}\")\n",
        "    epoch_dataset_length = []\n",
        "    #train the model\n",
        "    train_loss, train_acc = train(model, train_dataset, optimizer, criterion)\n",
        "    \n",
        "    #evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, test_dataset, criterion)\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(\"-------------------------------------------------------------------\")\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "    print(\"-------------------------------------------------------------------\")\n",
        "\n",
        "torch.save(model.state_dict(), os.path.join(\"/content/conLLmodel.pth\"))\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1: loading Train/Test/Validation datasets...\n",
            "****************************************************************************************************\n",
            "loading dictionaries...\n",
            "done\n",
            "Performing dataset pre-processing activities...\n",
            "loading dataset: ConLL2003-bioes-valid.txt\n",
            "done\n",
            "Parsing the dataset now...\n",
            "converting tokens to indices to tensors\n",
            "done\n",
            "Length matches! Hurray!\n",
            "****************************************************************************************************\n",
            "loading dictionaries...\n",
            "done\n",
            "Performing dataset pre-processing activities...\n",
            "loading dataset: ConLL2003-bioes-test.txt\n",
            "done\n",
            "Parsing the dataset now...\n",
            "converting tokens to indices to tensors\n",
            "done\n",
            "Length matches! Hurray!\n",
            "****************************************************************************************************\n",
            "loading dictionaries...\n",
            "done\n",
            "Performing dataset pre-processing activities...\n",
            "loading dataset: ConLL2003-bioes-train.txt\n",
            "done\n",
            "Parsing the dataset now...\n",
            "converting tokens to indices to tensors\n",
            "done\n",
            "Length matches! Hurray!\n",
            "****************************************************************************************************\n",
            "All datasets successfully loaded!\n",
            "loading dictionaries...\n",
            "done\n",
            "Performing dataset pre-processing activities...\n",
            "loading dataset: ConLL2003-bioes-valid.txt\n",
            "done\n",
            "Parsing the dataset now...\n",
            "converting tokens to indices to tensors\n",
            "done\n",
            "Length matches! Hurray!\n",
            "loading dictionaries...\n",
            "done\n",
            "Length of vocabulary is: 30291 and Length of POS table is: 47\n",
            "Length of Target look-up table is: 33\n",
            "Our vocab size to the model is therefore: 30340\n",
            "Step 02. builing the model...\n",
            "----------------------------------------------------------------\n",
            "Done! here is our model:\n",
            "RNNBIOESTagger(\n",
            "  (embedding): Embedding(30340, 100)\n",
            "  (lstm): LSTM(100, 64, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  (fc): Linear(in_features=128, out_features=34, bias=True)\n",
            "  (activation_fn): Tanh()\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "Epoch #: 0\n",
            "19:08:49\n",
            "19:09:09\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.947 | Train Acc: 68.02%\n",
            "\t Val. Loss: 1.864 |  Val. Acc: 70.22%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 1\n",
            "19:09:10\n",
            "19:09:29\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.837 | Train Acc: 70.23%\n",
            "\t Val. Loss: 1.831 |  Val. Acc: 76.93%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 2\n",
            "19:09:31\n",
            "19:09:50\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.798 | Train Acc: 84.88%\n",
            "\t Val. Loss: 1.805 |  Val. Acc: 92.32%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 3\n",
            "19:09:51\n",
            "19:10:11\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.769 | Train Acc: 95.06%\n",
            "\t Val. Loss: 1.792 |  Val. Acc: 93.36%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 4\n",
            "19:10:12\n",
            "19:10:32\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.754 | Train Acc: 95.97%\n",
            "\t Val. Loss: 1.795 |  Val. Acc: 93.54%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 5\n",
            "19:10:33\n",
            "19:10:52\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.749 | Train Acc: 96.44%\n",
            "\t Val. Loss: 1.793 |  Val. Acc: 93.55%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 6\n",
            "19:10:53\n",
            "19:11:13\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.746 | Train Acc: 96.72%\n",
            "\t Val. Loss: 1.792 |  Val. Acc: 93.81%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 7\n",
            "19:11:15\n",
            "19:11:34\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.744 | Train Acc: 96.93%\n",
            "\t Val. Loss: 1.793 |  Val. Acc: 93.92%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 8\n",
            "19:11:35\n",
            "19:11:55\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.742 | Train Acc: 97.09%\n",
            "\t Val. Loss: 1.794 |  Val. Acc: 93.76%\n",
            "-------------------------------------------------------------------\n",
            "Epoch #: 9\n",
            "19:11:56\n",
            "19:12:15\n",
            "-------------------------------------------------------------------\n",
            "\tTrain Loss: 1.742 | Train Acc: 97.22%\n",
            "\t Val. Loss: 1.795 |  Val. Acc: 93.97%\n",
            "-------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-981b5b4c9f68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content/conLLmodel.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;31m# print(y.keys())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'content/conLLmodel.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZghsEwiwiO-e"
      },
      "source": [
        "## 05. Predictions and Model Accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOXqcfcsiIMu",
        "outputId": "d526187d-31be-4890-a6e7-fc53b39fb774"
      },
      "source": [
        "############################################################################################\n",
        "################################## 07. Model Predictions #####################################\n",
        "# from modelConLL2003 import RNNBIOESTagger\n",
        "# from datasetConLL2003 import SlidingWindowDataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "ds = SlidingWindowDataset(\"/content/ConLL2003-bioes-valid.txt\")\n",
        "\n",
        "x1, x2, y = ds.load_dictionaries()\n",
        "\n",
        "# read this seq2seq model: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html --> for understanding embedding dimension and output dimension  \n",
        "VOCAB_SIZE = len(x1)+len(x2)+2\n",
        "EMBED_DIM = 100\n",
        "HIDDEN_DIM = 64\n",
        "NUM_LAYERS = 2\n",
        "NUM_OF_CLASSES = len(y)+1\n",
        "N_EPOCHS = 10\n",
        "LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "print(f\"Our vocab size to the model is therefore: {VOCAB_SIZE}\")\n",
        "################################### 02. NN Model  ########################################\n",
        "\n",
        "print(\"Step 02. builing the model...\")\n",
        "model = RNNBIOESTagger(embedding_dimension= EMBED_DIM,\n",
        "                            vocabulary_size=VOCAB_SIZE,\n",
        "                            hidden_dimension=HIDDEN_DIM,\n",
        "                            num_of_layers=NUM_LAYERS,\n",
        "                            dropout=0.2,\n",
        "                            output_dimension=NUM_OF_CLASSES)\n",
        "print(\"----------------------------------------------------------------\")\n",
        "print(\"Done! here is our model:\")\n",
        "print(model)\n",
        "print(\"----------------------------------------------------------------\")\n",
        "\n",
        "# load trained model\n",
        "model.load_state_dict(torch.load(\"/content/conLLmodel.pth\"))\n",
        "model.eval()\n",
        "\n",
        "idx_to_BIOES = {}\n",
        "print(\"Lets make predictions\")\n",
        "\n",
        "validation_dataset = DataLoader(dataset=SlidingWindowDataset(\"/content/ConLL2003-bioes-valid.txt\"),\n",
        "                                batch_size=64,\n",
        "                                shuffle=True)\n",
        "\n",
        "for key, value in y.items():\n",
        "    idx_to_BIOES[value] = key\n",
        "\n",
        "# print(idx_to_BIOES)\n",
        "\n",
        "def predict(sentence, model):\n",
        "\n",
        "    # token idx to tensor conversion\n",
        "\n",
        "    idx_to_torch01 = torch.tensor(sentence, dtype=torch.int64)\n",
        "    idx_to_torch = idx_to_torch01.unsqueeze(1).T\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(idx_to_torch)\n",
        "        predicted_ouput=torch.argmax(output,dim=2)\n",
        "        \n",
        "        predicted_labels = []\n",
        "\n",
        "        for pred in predicted_ouput:\n",
        "            for i in pred:\n",
        "                predicted_labels.append(idx_to_BIOES[int(i)])\n",
        "\n",
        "        return output, predicted_labels\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "# ==============================================================================\n",
        "# ACCURACY & PRECISION CALCULATIONS\n",
        "\n",
        "total_accuracy = []\n",
        "length_of_sentence = []\n",
        "\n",
        "def model_accuracy_precision():\n",
        "\n",
        "    \"\"\" returns accuracy of the model using the validation dataset.\"\"\"\n",
        "\n",
        "    for idx, (sample,actualy) in enumerate(validation_dataset):\n",
        "\n",
        "\n",
        "        for x,y in zip(sample,actualy):\n",
        "\n",
        "            labelsy = torch.squeeze(y,dim=-1)  #actual labels\n",
        "            probsy, predictedy = predict(x, model) #predicted labels\n",
        "            \n",
        "\n",
        "            actual_labels = []\n",
        "            for idx in labelsy:\n",
        "                actual_labels.append(idx_to_BIOES[int(idx)])\n",
        "                \n",
        "\n",
        "            correct = np.array(actual_labels) == np.array(predictedy) # boolean comparison\n",
        "\n",
        "            total_accuracy.append(correct.sum())\n",
        "            length_of_sentence.append(len(x))\n",
        "\n",
        "            acc=sum(total_accuracy)/sum(length_of_sentence)*100\n",
        "\n",
        "    return round(acc,2)\n",
        "\n",
        "print(f\"Total Accuracy of our model is: {model_accuracy_precision()}%\")\n",
        "# ======================================================================================\n",
        "\n",
        "for idx, (sample, label) in enumerate(validation_dataset):\n",
        "\n",
        "    if idx > 0:\n",
        "    \n",
        "        break\n",
        "\n",
        "print(\"One example:\")\n",
        "actual_labels, actual_sentence = [], []\n",
        "for idx in label[0]:\n",
        "    actual_labels.append(idx_to_BIOES[int(idx)])\n",
        "\n",
        "print(actual_labels)\n",
        "example = sample[0]\n",
        "probsy, predictions = predict(example, model)\n",
        "probsy_np = probsy.cpu().detach().numpy()\n",
        "probsy_np =  np.squeeze(probsy_np, axis=0)\n",
        "\n",
        "print(predictions)\n",
        "# ====================================================================================\n",
        "# Export the results of our predictions and their corresponding probabilities.\n",
        "# This will be used as input to the viterbi algorithm\n",
        "\n",
        "# Step 1: Export our BIOES predictions\n",
        "# # FILEPATH = \"C:/Users/rahin/projects/paper-draft-03/data/processed\"\n",
        "\n",
        "# textfile = open(FILEPATH+\"/sentence.txt\", \"w\")\n",
        "# for element in predictions:\n",
        "#     textfile.write(element + \"\\n\")\n",
        "# textfile.close()\n",
        "\n",
        "# # Step 2: Export the individual probability of each BIOES tag, given each words+POS tags predictions\n",
        "\n",
        "# np.save(\"/contents/tags_probabilities01.npy\", probsy_np)\n",
        "\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading dictionaries...\n",
            "done\n",
            "Performing dataset pre-processing activities...\n",
            "loading dataset: ConLL2003-bioes-valid.txt\n",
            "done\n",
            "Parsing the dataset now...\n",
            "converting tokens to indices to tensors\n",
            "done\n",
            "Length matches! Hurray!\n",
            "loading dictionaries...\n",
            "done\n",
            "Our vocab size to the model is therefore: 30340\n",
            "Step 02. builing the model...\n",
            "----------------------------------------------------------------\n",
            "Done! here is our model:\n",
            "RNNBIOESTagger(\n",
            "  (embedding): Embedding(30340, 100)\n",
            "  (lstm): LSTM(100, 64, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  (fc): Linear(in_features=128, out_features=34, bias=True)\n",
            "  (activation_fn): Tanh()\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "Lets make predictions\n",
            "loading dictionaries...\n",
            "done\n",
            "Performing dataset pre-processing activities...\n",
            "loading dataset: ConLL2003-bioes-valid.txt\n",
            "done\n",
            "Parsing the dataset now...\n",
            "converting tokens to indices to tensors\n",
            "done\n",
            "Length matches! Hurray!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total Accuracy of our model is: 94.27%\n",
            "One example:\n",
            "['B-NP', 'B-PP', 'B-NP', 'I-NP', 'B-NP', 'I-NP', 'B-VP', 'B-PP', 'B-NP', 'I-NP', 'B-VP', 'B-PP', 'B-NP', 'I-NP', 'O', 'B-NP', 'E-VP', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING']\n",
            "['B-NP', 'B-PP', 'B-NP', 'I-NP', 'B-NP', 'I-NP', 'B-VP', 'B-PP', 'B-NP', 'I-NP', 'B-VP', 'B-PP', 'B-NP', 'I-NP', 'O', 'B-NP', 'B-VP', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sFS602ejB5U",
        "outputId": "15c4937e-87af-4ec2-8e4a-c8063dbc30ac"
      },
      "source": [
        "for idx, (sample, label) in enumerate(validation_dataset):\n",
        "\n",
        "    if idx > 0:\n",
        "    \n",
        "        break\n",
        "\n",
        "print(\"One example:\")\n",
        "actual_labels, actual_sentence = [], []\n",
        "for idx in label[0]:\n",
        "    actual_labels.append(idx_to_BIOES[int(idx)])\n",
        "\n",
        "print(actual_labels)\n",
        "example = sample[0]\n",
        "probsy, predictions = predict(example, model)\n",
        "probsy_np = probsy.cpu().detach().numpy()\n",
        "probsy_np =  np.squeeze(probsy_np, axis=0)\n",
        "\n",
        "print(predictions)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One example:\n",
            "['-X-', 'B-NP', 'B-VP', 'B-NP', 'I-NP', 'E-NP', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING']\n",
            "['B-NP', 'B-NP', 'I-NP', 'I-NP', 'I-NP', 'E-NP', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING', 'PADDING']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}